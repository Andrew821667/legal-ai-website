# –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≥–∞–π–¥ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ SEO AI Models –≤ Legal AI

**–î–∞—Ç–∞:** 11 –Ω–æ—è–±—Ä—è 2025  
**–°—Ç–∞—Ç—É—Å:** –ì–æ—Ç–æ–≤–æ –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

---

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (15 –º–∏–Ω—É—Ç)

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –ü–µ—Ä–µ–π—Ç–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞
cd /Users/andrew/legal-ai-bot/website

# –°–æ–∑–¥–∞—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ (–µ—Å–ª–∏ –µ—â–µ –Ω–µ—Ç)
python3 -m venv venv
source venv/bin/activate

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ SEO –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
pip install requests beautifulsoup4 lxml markdown
pip install numpy scikit-learn
pip install spacy nltk
pip install playwright
pip install google-search-results
pip install tqdm colorlog

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä—ã –¥–ª—è Playwright
python -m playwright install chromium
```

### 2. –ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç (seo_test.py)

```python
#!/usr/bin/env python3
"""–ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç SEO –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è Legal AI"""

import sys
sys.path.insert(0, '/tmp/seo-ai-models')

from seo_ai_models.models.seo_advisor.advisor import SEOAdvisor

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π –æ—Ç—Ä–∞—Å–ª–∏
advisor = SEOAdvisor(industry='legal')

# –¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
test_content = """
# –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è –ø–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –ø—Ä–∞–≤—É

## –í–≤–µ–¥–µ–Ω–∏–µ

–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–∞–≤–æ - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å —é—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –ø—Ä–∞–≤–æ–≤—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏, 
—Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è–º–∏...

## –û—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã

–ù–∞—à–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∞—è —Ñ–∏—Ä–º–∞ –∏–º–µ–µ—Ç –±–æ–ª–µ–µ 15 –ª–µ—Ç –æ–ø—ã—Ç–∞ –≤ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∫–æ–º–ø–∞–Ω–∏–π...

## –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã

–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏?
–°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–Ω–∏–º–∞–µ—Ç –ø—Ä–æ—Ü–µ–¥—É—Ä–∞?

## –ö–æ–Ω—Ç–∞–∫—Ç—ã

–°–≤—è–∂–∏—Ç–µ—Å—å —Å –Ω–∞—à–µ–π –∫–æ–º–∞–Ω–¥–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏.
"""

# –í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑
try:
    report = advisor.analyze_content(
        content=test_content,
        keywords=['–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–∞–≤–æ', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '—é—Ä–∏–¥–∏—á–µ—Å–∫–∞—è —Ñ–∏—Ä–º–∞']
    )
    
    print("‚úÖ –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω!")
    print(f"\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è: {report.predicted_position}")
    print(f"E-E-A-T –û—Ü–µ–Ω–∫–∞: {report.content_quality}")
    print(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {report.recommendations}")
    
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
    import traceback
    traceback.print_exc()
```

**–ó–∞–ø—É—Å–∫:**
```bash
python seo_test.py
```

---

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–∞—à–∏–º Next.js —Å–∞–π—Ç–æ–º

### 1. –ö—Ä–∞—É–ª–∏–Ω–≥ —Å–∞–π—Ç–∞ (site_analyzer.py)

```python
#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü Legal AI —Å–∞–π—Ç–∞
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Playwright –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Next.js
"""

import sys
import json
from pathlib import Path
sys.path.insert(0, '/tmp/seo-ai-models')

from seo_ai_models.parsers.unified.unified_parser import UnifiedParser
from seo_ai_models.models.seo_advisor.advisor import SEOAdvisor
from datetime import datetime

class LegalAIAnalyzer:
    def __init__(self, site_url: str = "https://legal-ai.com"):
        self.site_url = site_url
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è Next.js
        self.parser = UnifiedParser(
            force_spa_mode=True,
            auto_detect_spa=True,
            parallel_parsing=True,
            max_workers=3,  # –ù–µ –Ω–∞–≥—Ä—É–∂–∞—Ç—å —Å–µ—Ä–≤–µ—Ä
            
            spa_settings={
                "headless": True,
                "wait_for_idle": 2000,
                "wait_for_timeout": 10000,
                "browser_type": "chromium",
                "intercept_ajax": True
            },
            extract_semantic=True
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è legal —Å–µ–≥–º–µ–Ω—Ç–∞
        self.advisor = SEOAdvisor(industry='legal')
        
    def analyze_site(self, max_pages: int = 20) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ —Å–∞–π—Ç–∞"""
        print(f"–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ {self.site_url}...")
        print(f"–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages}\n")
        
        # –ö—Ä–∞—É–ª–∏–Ω–≥ —Å–∞–π—Ç–∞
        print("1. –ö—Ä–∞—É–ª–∏–Ω–≥ —Å–∞–π—Ç–∞...")
        site_analysis = self.parser.crawl_site(
            self.site_url,
            max_pages=max_pages,
            delay=1.5  # –ó–∞–¥–µ—Ä–∂–∫–∞ 1.5 —Å–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        )
        
        if not site_analysis.get("success"):
            print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫—Ä–∞—É–ª–∏–Ω–≥–µ —Å–∞–π—Ç–∞")
            return {"success": False, "error": "–ö—Ä–∞—É–ª–∏–Ω–≥ –Ω–µ —É—Å–ø–µ—à–µ–Ω"}
        
        pages_data = site_analysis.get("site_data", {}).get("pages", {})
        print(f"‚úÖ –ü—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages_data)}\n")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print("2. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        results = []
        
        for idx, (url, page_data) in enumerate(pages_data.items(), 1):
            print(f"   [{idx}/{len(pages_data)}] {url}")
            
            try:
                # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                content_text = page_data.get("text", "")
                if not content_text or len(content_text) < 50:
                    print(f"       ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
                    continue
                
                # –ê–Ω–∞–ª–∏–∑ SEO
                analysis = self.advisor.analyze_content(
                    content=content_text,
                    url=url
                )
                
                result = {
                    "url": url,
                    "title": page_data.get("title", ""),
                    "word_count": page_data.get("word_count", 0),
                    "predicted_position": analysis.predicted_position,
                    "eeat_score": analysis.content_quality.eeat_score,
                    "semantic_depth": analysis.feature_scores.get("semantic_depth", 0),
                    "readability": analysis.feature_scores.get("readability_score", 0),
                    "strengths": analysis.content_quality.strengths[:2],
                    "weaknesses": analysis.content_quality.weaknesses[:2],
                    "critical_issues": [r for r in analysis.recommendations.get("critical", [])],
                }
                results.append(result)
                
            except Exception as e:
                print(f"       ‚ùå –û—à–∏–±–∫–∞: {str(e)[:50]}")
                continue
        
        print(f"\n‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(results)}\n")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        report = {
            "scan_date": datetime.now().isoformat(),
            "site_url": self.site_url,
            "total_pages": len(results),
            "pages": results,
            "summary": self._generate_summary(results)
        }
        
        return report
    
    def _generate_summary(self, results: list) -> dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not results:
            return {}
        
        positions = [r["predicted_position"] for r in results]
        eeat_scores = [r["eeat_score"] for r in results]
        word_counts = [r["word_count"] for r in results]
        
        return {
            "avg_predicted_position": sum(positions) / len(positions),
            "avg_eeat_score": sum(eeat_scores) / len(eeat_scores),
            "avg_word_count": sum(word_counts) / len(word_counts),
            "pages_in_top_10": len([p for p in positions if p <= 10]),
            "pages_in_top_20": len([p for p in positions if p <= 20]),
            "pages_below_quality": len([e for e in eeat_scores if e < 0.65]),
            "critical_issues_total": sum(len(r.get("critical_issues", [])) for r in results)
        }
    
    def save_report(self, report: dict, filename: str = "seo_report.json"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª"""
        output_path = Path(filename)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    
    def print_summary(self, report: dict):
        """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ –≤ –∫–æ–Ω—Å–æ–ª—å"""
        summary = report.get("summary", {})
        
        print("\n" + "="*60)
        print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("="*60)
        print(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {report.get('total_pages', 0)}")
        print(f"–°—Ä–µ–¥–Ω—è—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è: {summary.get('avg_predicted_position', 0):.1f}")
        print(f"–°—Ä–µ–¥–Ω–∏–π E-E-A-T —Åcore: {summary.get('avg_eeat_score', 0):.2f}")
        print(f"–°—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {summary.get('avg_word_count', 0):.0f} —Å–ª–æ–≤")
        print(f"–°—Ç—Ä–∞–Ω–∏—Ü –≤ —Ç–æ–ø-10: {summary.get('pages_in_top_10', 0)}")
        print(f"–°—Ç—Ä–∞–Ω–∏—Ü –≤ —Ç–æ–ø-20: {summary.get('pages_in_top_20', 0)}")
        print(f"–°—Ç—Ä–∞–Ω–∏—Ü –Ω–∏–∂–µ –∫–∞—á–µ—Å—Ç–≤–∞ (E-E-A-T < 0.65): {summary.get('pages_below_quality', 0)}")
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º: {summary.get('critical_issues_total', 0)}")
        print("="*60 + "\n")

def main():
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    SITE_URL = "https://legal-ai.com"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à —Å–∞–π—Ç
    MAX_PAGES = 20  # –î–ª—è —Ç–µ—Å—Ç–∞ - 20 —Å—Ç—Ä–∞–Ω–∏—Ü
    
    # –ê–Ω–∞–ª–∏–∑
    analyzer = LegalAIAnalyzer(SITE_URL)
    report = analyzer.analyze_site(max_pages=MAX_PAGES)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥
    if report.get("success", True):
        analyzer.save_report(report, "seo_report_latest.json")
        analyzer.print_summary(report)
        
        # –í—ã–≤–æ–¥ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º
        print("\n–¢–û–ü –ü–†–û–ë–õ–ï–ú –î–õ–Ø –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:")
        print("-" * 60)
        for page in report.get("pages", [])[:5]:  # –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–∞–Ω–∏—Ü
            if page["critical_issues"]:
                print(f"\n{page['url']}")
                for issue in page["critical_issues"][:2]:
                    print(f"  ‚Ä¢ {issue}")

if __name__ == "__main__":
    main()
```

**–ó–∞–ø—É—Å–∫:**
```bash
python site_analyzer.py
```

---

## CI/CD –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### GitHub Actions –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

```yaml
# .github/workflows/seo-daily-check.yml
name: Daily SEO Check

on:
  schedule:
    - cron: '0 2 * * *'  # –ï–∂–µ–¥–Ω–µ–≤–Ω–æ –≤ 2:00 UTC
  workflow_dispatch:     # –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫

jobs:
  seo-analysis:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 lxml markdown numpy scikit-learn
        pip install spacy nltk gensim playwright google-search-results tqdm colorlog
        python -m playwright install chromium
    
    - name: Clone SEO AI Models
      run: |
        git clone https://github.com/Andrew821667/seo-ai-models.git /tmp/seo-ai-models || true
    
    - name: Run SEO Analysis
      run: |
        python scripts/site_analyzer.py
    
    - name: Check Results
      run: |
        python scripts/check_seo_quality.py --report seo_report_latest.json
    
    - name: Upload Report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: seo-report-${{ github.run_id }}
        path: seo_report_latest.json
        retention-days: 30
    
    - name: Slack Notification
      if: failure()
      uses: slackapi/slack-github-action@v1
      with:
        webhook-url: ${{ secrets.SLACK_WEBHOOK }}
        payload: |
          {
            "text": "SEO Analysis Failed",
            "details": "Check https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          }
```

### –°–∫—Ä–∏–ø—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

```python
#!/usr/bin/env python3
"""check_seo_quality.py - –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""

import json
import sys
from pathlib import Path

# –ü–æ—Ä–æ–≥–∏ –¥–ª—è Legal AI (YMYL —Å–∞–π—Ç)
THRESHOLDS = {
    "eeat_score": 0.65,           # –ú–∏–Ω–∏–º—É–º –¥–ª—è legal
    "predicted_position": 25,     # –¶–µ–ª–µ–≤–∞—è –ø–æ–∑–∏—Ü–∏—è <= 25
    "word_count": 800,            # –ú–∏–Ω–∏–º—É–º —Å–ª–æ–≤ –¥–ª—è —Å—Ç–∞—Ç—å–∏
    "readability": 50,            # Flesch Reading Ease >= 50
}

def check_report(report_file: str):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—á–µ—Ç–∞ –ø—Ä–æ—Ç–∏–≤ –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
    
    with open(report_file, 'r') as f:
        report = json.load(f)
    
    issues = []
    pages = report.get("pages", [])
    
    print("\n–ü–†–û–í–ï–†–ö–ê –ü–û–†–û–ì–û–í–´–• –ó–ù–ê–ß–ï–ù–ò–ô")
    print("="*60)
    
    for page in pages:
        url = page.get("url", "unknown")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ E-E-A-T
        eeat = page.get("eeat_score", 0)
        if eeat < THRESHOLDS["eeat_score"]:
            issues.append(f"‚ö†Ô∏è {url}: E-E-A-T {eeat:.2f} < {THRESHOLDS['eeat_score']}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∑–∏—Ü–∏–∏
        position = page.get("predicted_position", 100)
        if position > THRESHOLDS["predicted_position"]:
            issues.append(f"‚ö†Ô∏è {url}: –ü–æ–∑–∏—Ü–∏—è {position} > {THRESHOLDS['predicted_position']}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä–µ–º–∞
        words = page.get("word_count", 0)
        if words < THRESHOLDS["word_count"]:
            issues.append(f"‚ö†Ô∏è {url}: –°–ª–æ–≤ {words} < {THRESHOLDS['word_count']}")
    
    if issues:
        print(f"\n–ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {len(issues)}\n")
        for issue in issues[:10]:  # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 10
            print(issue)
        print("\n" + "="*60)
        sys.exit(1)
    else:
        print("‚úÖ –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º!")
        print("="*60)
        sys.exit(0)

if __name__ == "__main__":
    report_file = sys.argv[1] if len(sys.argv) > 1 else "seo_report_latest.json"
    check_report(report_file)
```

---

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ

### –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ—Ç—á–µ—Ç–æ–≤

```python
#!/usr/bin/env python3
"""–ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""

import json
import shutil
from pathlib import Path
from datetime import datetime

def archive_report(report_file: str):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ –∞—Ä—Ö–∏–≤"""
    
    report_path = Path(report_file)
    if not report_path.exists():
        print(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {report_file}")
        return
    
    # –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∞—Ä—Ö–∏–≤–∞
    archive_dir = Path("seo_reports/history")
    archive_dir.mkdir(parents=True, exist_ok=True)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–º—è —Ñ–∞–π–ª–∞ —Å –¥–∞—Ç–æ–π
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    archive_name = f"seo_report_{timestamp}.json"
    archive_path = archive_dir / archive_name
    
    # –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª
    shutil.copy2(report_path, archive_path)
    
    print(f"‚úÖ –û—Ç—á–µ—Ç –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω: {archive_path}")
    
    # –°–æ–∑–¥–∞—Ç—å —Ç–∞–∫–∂–µ —Å—Å—ã–ª–∫—É –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç—á–µ—Ç
    latest_path = Path("seo_reports/latest.json")
    if latest_path.exists():
        latest_path.unlink()
    shutil.copy2(report_path, latest_path)
    print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π: {latest_path}")

if __name__ == "__main__":
    archive_report("seo_report_latest.json")
```

---

## –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
python -c "
import sys
sys.path.insert(0, '/tmp/seo-ai-models')
from seo_ai_models.models.seo_advisor.advisor import SEOAdvisor

advisor = SEOAdvisor(industry='legal')
content = '–í–∞—à –∫–æ–Ω—Ç–µ–Ω—Ç –∑–¥–µ—Å—å...'
report = advisor.analyze_content(content)
print(f'–ü–æ–∑–∏—Ü–∏—è: {report.predicted_position}')
"

# –ü—Ä–æ—Å–º–æ—Ç—Ä –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ—Ç—á–µ—Ç–∞
cat seo_report_latest.json | jq '.summary'

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –æ—Ç—á–µ—Ç–æ–º
diff <(jq '.summary' seo_reports/history/seo_report_2025-11-10*.json) \
     <(jq '.summary' seo_report_latest.json)
```

---

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ—Å–ª–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

```
legal-ai-bot/website/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ site_analyzer.py           # –ì–ª–∞–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
‚îÇ   ‚îú‚îÄ‚îÄ check_seo_quality.py       # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
‚îÇ   ‚îú‚îÄ‚îÄ archive_reports.py         # –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ
‚îÇ   ‚îî‚îÄ‚îÄ requirements_seo.txt       # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ SEO
‚îÇ
‚îú‚îÄ‚îÄ seo_reports/
‚îÇ   ‚îú‚îÄ‚îÄ latest.json                # –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç—á–µ—Ç
‚îÇ   ‚îî‚îÄ‚îÄ history/
‚îÇ       ‚îú‚îÄ‚îÄ seo_report_2025-11-11_02-00-00.json
‚îÇ       ‚îú‚îÄ‚îÄ seo_report_2025-11-10_02-00-00.json
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ .github/workflows/
‚îÇ   ‚îî‚îÄ‚îÄ seo-daily-check.yml        # GitHub Actions
‚îÇ
‚îú‚îÄ‚îÄ SEO_AI_MODELS_REPORT.md         # –≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç
‚îú‚îÄ‚îÄ INTEGRATION_GUIDE.md            # –ì–∞–π–¥ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
‚îî‚îÄ‚îÄ README.md
```

---

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –¥–µ–ø–ª–æ–µ–º

```python
#!/usr/bin/env python3
"""–ü—Ä–æ–≤–µ—Ä–∫–∞ SEO –ø–µ—Ä–µ–¥ –¥–µ–ø–ª–æ–µ–º –≤ production"""

import subprocess
import sys

def pre_deploy_seo_check():
    """–í—ã–ø–æ–ª–Ω–∏—Ç—å SEO –ø—Ä–æ–≤–µ—Ä–∫—É –ø–µ—Ä–µ–¥ –¥–µ–ø–ª–æ–µ–º"""
    
    print("üîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SEO –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ–¥ –¥–µ–ø–ª–æ–µ–º...\n")
    
    # 1. –ê–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞
    print("1Ô∏è‚É£  –ê–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞...")
    result = subprocess.run([sys.executable, "scripts/site_analyzer.py"], 
                          capture_output=True)
    if result.returncode != 0:
        print("‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞")
        sys.exit(1)
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    print("\n2Ô∏è‚É£  –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞...")
    result = subprocess.run([sys.executable, "scripts/check_seo_quality.py"],
                          capture_output=True)
    if result.returncode != 0:
        print("‚ùå –°–∞–π—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞")
        print(result.stderr.decode())
        sys.exit(1)
    
    # 3. –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\n3Ô∏è‚É£  –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞...")
    subprocess.run([sys.executable, "scripts/archive_reports.py"])
    
    print("\n‚úÖ SEO –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    print("–ì–æ—Ç–æ–≤–æ –∫ –¥–µ–ø–ª–æ—é.")
    
    return True

if __name__ == "__main__":
    pre_deploy_seo_check()
```

---

## –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏ –æ—Ç–ª–∞–¥–∫–∞

### –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('seo_analysis.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

### –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

```python
try:
    result = analyzer.analyze_site(max_pages=20)
except TimeoutError:
    print("‚è±Ô∏è –¢–∞–π–º-–∞—É—Ç –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ (–ø—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è)")
except ConnectionError:
    print("üåê –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Å–∞–π—Ç—É")
except Exception as e:
    print(f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
```

