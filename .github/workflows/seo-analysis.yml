name: Automatic SEO Analysis

on:
  # –ó–∞–ø—É—Å–∫ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é (–∫–∞–∂–¥–æ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ –≤ 03:00 UTC)
  schedule:
    - cron: '0 3 * * 0'

  # –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤—Ä—É—á–Ω—É—é
  workflow_dispatch:

  # –ó–∞–ø—É—Å–∫ –ø–æ—Å–ª–µ –¥–µ–ø–ª–æ—è –≤ production
  push:
    branches:
      - main
    paths:
      - 'app/**'
      - 'components/**'
      - 'public/**'

jobs:
  seo-analysis:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install playwright beautifulsoup4 requests openai
          playwright install chromium

      - name: Run SEO analysis
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          mkdir -p seo-reports

          cat > analyze_seo.py << 'EOF'
          import asyncio
          import json
          import os
          from datetime import datetime
          from playwright.async_api import async_playwright
          from bs4 import BeautifulSoup

          SITE_URL = "https://legal-ai-website-iota.vercel.app"

          async def analyze_page(url):
              """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
              async with async_playwright() as p:
                  browser = await p.chromium.launch()
                  page = await browser.new_page()

                  try:
                      await page.goto(url, wait_until='networkidle', timeout=30000)
                      content = await page.content()

                      soup = BeautifulSoup(content, 'html.parser')

                      # SEO –º–µ—Ç—Ä–∏–∫–∏
                      title = soup.find('title')
                      meta_desc = soup.find('meta', attrs={'name': 'description'})
                      h1_tags = soup.find_all('h1')
                      h2_tags = soup.find_all('h2')
                      images = soup.find_all('img')
                      links = soup.find_all('a')

                      # –ü–æ–¥—Å—á—ë—Ç —Å–ª–æ–≤
                      text = soup.get_text()
                      words = len(text.split())

                      # –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                      images_without_alt = [img for img in images if not img.get('alt')]

                      # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                      internal_links = [link for link in links if link.get('href', '').startswith('/') or SITE_URL in link.get('href', '')]

                      result = {
                          "url": url,
                          "title": title.text if title else None,
                          "title_length": len(title.text) if title else 0,
                          "meta_description": meta_desc.get('content') if meta_desc else None,
                          "meta_desc_length": len(meta_desc.get('content', '')) if meta_desc else 0,
                          "h1_count": len(h1_tags),
                          "h1_texts": [h1.text.strip() for h1 in h1_tags],
                          "h2_count": len(h2_tags),
                          "word_count": words,
                          "images_total": len(images),
                          "images_without_alt": len(images_without_alt),
                          "internal_links": len(internal_links),
                          "total_links": len(links),
                      }

                      # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ score
                      score = 0
                      max_score = 100

                      # Title (20 points)
                      if result['title'] and 30 <= result['title_length'] <= 60:
                          score += 20
                      elif result['title']:
                          score += 10

                      # Meta description (15 points)
                      if result['meta_description'] and 120 <= result['meta_desc_length'] <= 160:
                          score += 15
                      elif result['meta_description']:
                          score += 8

                      # H1 (15 points)
                      if result['h1_count'] == 1:
                          score += 15
                      elif result['h1_count'] > 0:
                          score += 8

                      # Content (20 points)
                      if result['word_count'] >= 300:
                          score += 20
                      elif result['word_count'] >= 150:
                          score += 10

                      # Images alt (15 points)
                      if result['images_total'] > 0:
                          alt_ratio = 1 - (result['images_without_alt'] / result['images_total'])
                          score += int(15 * alt_ratio)
                      else:
                          score += 15

                      # Internal links (15 points)
                      if result['internal_links'] >= 5:
                          score += 15
                      elif result['internal_links'] >= 3:
                          score += 10
                      elif result['internal_links'] >= 1:
                          score += 5

                      result['score'] = min(score, max_score)

                      # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                      recommendations = []

                      if not result['title']:
                          recommendations.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç <title> —Ç–µ–≥")
                      elif result['title_length'] < 30:
                          recommendations.append("‚ö†Ô∏è Title —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 30-60 —Å–∏–º–≤–æ–ª–æ–≤)")
                      elif result['title_length'] > 60:
                          recommendations.append("‚ö†Ô∏è Title —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 30-60 —Å–∏–º–≤–æ–ª–æ–≤)")

                      if not result['meta_description']:
                          recommendations.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta description")
                      elif result['meta_desc_length'] < 120:
                          recommendations.append("‚ö†Ô∏è Meta description —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 120-160 —Å–∏–º–≤–æ–ª–æ–≤)")
                      elif result['meta_desc_length'] > 160:
                          recommendations.append("‚ö†Ô∏è Meta description —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 120-160 —Å–∏–º–≤–æ–ª–æ–≤)")

                      if result['h1_count'] == 0:
                          recommendations.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç H1 –∑–∞–≥–æ–ª–æ–≤–æ–∫")
                      elif result['h1_count'] > 1:
                          recommendations.append("‚ö†Ô∏è –ù–µ—Å–∫–æ–ª—å–∫–æ H1 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–¥–∏–Ω)")

                      if result['word_count'] < 300:
                          recommendations.append(f"‚ö†Ô∏è –ú–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ({result['word_count']} —Å–ª–æ–≤, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 300+)")

                      if result['images_without_alt'] > 0:
                          recommendations.append(f"‚ö†Ô∏è {result['images_without_alt']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ alt —Ç–µ–∫—Å—Ç–∞")

                      if result['internal_links'] < 3:
                          recommendations.append(f"‚ö†Ô∏è –ú–∞–ª–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ ({result['internal_links']}, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 3+)")

                      result['recommendations'] = recommendations

                      await browser.close()
                      return result

                  except Exception as e:
                      await browser.close()
                      return {
                          "url": url,
                          "error": str(e),
                          "score": 0
                      }

          async def main():
              pages = [
                  SITE_URL + "/",
              ]

              results = []
              for page_url in pages:
                  print(f"Analyzing: {page_url}")
                  result = await analyze_page(page_url)
                  results.append(result)
                  print(f"Score: {result.get('score', 0)}/100")

              # –û–±—â–∏–π –æ—Ç—á—ë—Ç
              total_score = sum(r.get('score', 0) for r in results) / len(results) if results else 0

              report = {
                  "timestamp": datetime.now().isoformat(),
                  "site_url": SITE_URL,
                  "overall_score": round(total_score, 2),
                  "pages_analyzed": results
              }

              # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON
              report_file = f'seo-reports/report-{datetime.now().strftime("%Y-%m-%d")}.json'
              with open(report_file, 'w', encoding='utf-8') as f:
                  json.dump(report, f, ensure_ascii=False, indent=2)

              print(f"\n‚úÖ Analysis complete!")
              print(f"Overall Score: {report['overall_score']}/100")

              # Markdown summary
              with open('seo-reports/SUMMARY.md', 'w', encoding='utf-8') as f:
                  f.write(f"# üîç SEO Analysis Report\n\n")
                  f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                  f.write(f"**Overall Score:** {report['overall_score']}/100\n\n")

                  if report['overall_score'] >= 80:
                      f.write(f"**Status:** ‚úÖ –û—Ç–ª–∏—á–Ω–æ\n\n")
                  elif report['overall_score'] >= 60:
                      f.write(f"**Status:** ‚ö†Ô∏è –•–æ—Ä–æ—à–æ, –Ω–æ –µ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è\n\n")
                  else:
                      f.write(f"**Status:** ‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n\n")

                  f.write(f"## üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n\n")
                  for page in results:
                      if 'error' not in page:
                          f.write(f"### [{page['url']}]({page['url']})\n\n")
                          f.write(f"**Score:** {page['score']}/100\n\n")
                          f.write(f"**–ú–µ—Ç—Ä–∏–∫–∏:**\n")
                          f.write(f"- Title: {page['title_length']} —Å–∏–º–≤–æ–ª–æ–≤\n")
                          f.write(f"- Meta description: {page['meta_desc_length']} —Å–∏–º–≤–æ–ª–æ–≤\n")
                          f.write(f"- H1: {page['h1_count']}\n")
                          f.write(f"- –°–ª–æ–≤: {page['word_count']}\n")
                          f.write(f"- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {page['images_total']} (–±–µ–∑ alt: {page['images_without_alt']})\n")
                          f.write(f"- –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {page['internal_links']}\n\n")

                          if page['recommendations']:
                              f.write(f"**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n")
                              for rec in page['recommendations']:
                                  f.write(f"- {rec}\n")
                              f.write(f"\n")
                      else:
                          f.write(f"### ‚ùå {page['url']}\n\n")
                          f.write(f"Error: {page['error']}\n\n")

                  # GPT –∞–Ω–∞–ª–∏–∑ (–µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á)
                  if os.getenv('OPENAI_API_KEY'):
                      f.write(f"\n## ü§ñ AI –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n")
                      f.write(f"_GPT –∞–Ω–∞–ª–∏–∑ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ —Å–ª–µ–¥—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏_\n\n")

                  f.write(f"\n---\n\n")
                  f.write(f"*–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω–æ Legal AI PRO SEO Analyzer*\n")

          if __name__ == "__main__":
              asyncio.run(main())
          EOF

          python analyze_seo.py

      - name: Upload SEO report
        uses: actions/upload-artifact@v4
        with:
          name: seo-analysis-report
          path: seo-reports/
          retention-days: 90

      - name: Display Summary
        run: |
          echo "üìä SEO Analysis Summary:"
          cat seo-reports/SUMMARY.md

      - name: Create Issue with recommendations
        if: github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('seo-reports/SUMMARY.md', 'utf8');
            const reportFile = 'seo-reports/report-' + new Date().toISOString().split('T')[0] + '.json';

            let reportData = { overall_score: 0 };
            try {
              reportData = JSON.parse(fs.readFileSync(reportFile, 'utf8'));
            } catch (e) {
              console.log('Could not read report file');
            }

            const issueBody = `${summary}

            ## üìé –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç

            –ü–æ–ª–Ω—ã–π JSON –æ—Ç—á—ë—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –≤ [workflow artifacts](${context.payload.repository.html_url}/actions/runs/${context.runId}).

            ## üéØ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

            1. –ü—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤—ã—à–µ
            2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

            ---
            *–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω–æ SEO AI Analyzer*`;

            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `üìä SEO Report ${new Date().toLocaleDateString('ru-RU')} - Score: ${reportData.overall_score}/100`,
              body: issueBody,
              labels: ['seo', 'automated']
            });
