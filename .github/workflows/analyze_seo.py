#!/usr/bin/env python3
"""SEO Analysis Script for GitHub Actions - Fixed to work around ParsingPipeline bug"""

import json
import sys
from datetime import datetime
from dataclasses import asdict, is_dataclass
import requests
from bs4 import BeautifulSoup

sys.path.insert(0, 'seo-tools')

SITE_URL = 'https://legal-ai-website-iota.vercel.app'


def dataclass_to_dict(obj):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç dataclass –æ–±—ä–µ–∫—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
    if is_dataclass(obj):
        return asdict(obj)
    elif isinstance(obj, dict):
        return {k: dataclass_to_dict(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [dataclass_to_dict(item) for item in obj]
    else:
        return obj

def main():
    try:
        print('üîç –ò–º–ø–æ—Ä—Ç–∏—Ä—É—é –º–æ–¥—É–ª–∏...')
        from seo_ai_models.models.seo_advisor.advisor import SEOAdvisor
        from seo_ai_models.parsers.extractors.content_extractor import ContentExtractor
        from seo_ai_models.parsers.extractors.meta_extractor import MetaExtractor

        print('‚úÖ –ú–æ–¥—É–ª–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ')

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (–±–µ–∑ ParsingPipeline –∏–∑-–∑–∞ –±–∞–≥–∞)
        advisor = SEOAdvisor()
        content_extractor = ContentExtractor()
        meta_extractor = MetaExtractor()

        print(f'üåê –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å–∞–π—Ç: {SITE_URL}')

        # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ requests
        print('üì• –ó–∞–≥—Ä—É–∂–∞—é HTML...')
        response = requests.get(SITE_URL, headers={
            'User-Agent': 'Mozilla/5.0 (compatible; SEO-Analyzer/1.0)'
        }, timeout=30)
        response.raise_for_status()

        html_content = response.text
        status_code = response.status_code

        print(f'\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞:')
        print(f'  Status code: {status_code}')
        print(f'  Content length: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤')

        if status_code != 200:
            print(f'  ‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π status code: {status_code}')
            create_error_report({'error': f'HTTP {status_code}'})
            sys.exit(1)

        print(f'  ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞')

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ seo-ai-models —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã
        print('üî¨ –ò–∑–≤–ª–µ–∫–∞—é –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ...')
        content_analysis = content_extractor.extract_content(html_content, SITE_URL)
        meta_analysis = meta_extractor.extract_meta_information(html_content, SITE_URL)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è SEO –∞–Ω–∞–ª–∏–∑–∞
        content_text = content_analysis.get('content', {}).get('all_text', '')
        title = meta_analysis.get('meta_tags', {}).get('title', '')
        description = meta_analysis.get('meta_tags', {}).get('description', '')

        print(f'\nüìù –ù–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç:')
        print(f'  Title: {title[:80]}...' if len(title) > 80 else f'  Title: {title}')
        print(f'  Description: {description[:80]}...' if len(description) > 80 else f'  Description: {description}')
        print(f'  Content length: {len(content_text)} —Å–∏–º–≤–æ–ª–æ–≤')
        print(f'  Word count: {len(content_text.split())} —Å–ª–æ–≤')

        # SEO –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ advisor
        print(f'\nüî¨ –ó–∞–ø—É—Å–∫–∞—é SEO –∞–Ω–∞–ª–∏–∑...')

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ title –∏ content
        target_keywords = []
        if title:
            # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è title
            target_keywords.extend([w.lower() for w in title.split() if len(w) > 3])

        # –ï—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–µ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ AI
        if not target_keywords:
            target_keywords = ['legal', 'ai', '—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è']

        print(f'  –¶–µ–ª–µ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {", ".join(target_keywords[:5])}...')

        # –§–æ—Ä–º–∏—Ä—É–µ–º markdown-–∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è advisor
        markdown_content = f"# {title}\n\n{content_text}"
        seo_report = advisor.analyze_content(markdown_content, target_keywords)

        # seo_report —ç—Ç–æ SEOAnalysisReport dataclass, –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ
        predicted_position = float(seo_report.predicted_position) if seo_report.predicted_position else 50.0
        overall_score = max(0, min(100, int((1 - (predicted_position / 100)) * 100)))  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ score

        print(f'  Predicted Position: {predicted_position:.1f}')
        print(f'  SEO Score: {overall_score}/100')

        # –°–±–æ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π dataclass –≤ dict
        results = {
            'timestamp': datetime.now().isoformat(),
            'site_url': SITE_URL,
            'overall_score': overall_score,
            'predicted_position': predicted_position,
            'page_metadata': {
                'title': title,
                'title_length': len(title),
                'description': description,
                'description_length': len(description),
                'word_count': len(content_text.split()),
                'content_length': len(content_text),
                'h1_count': len(content_analysis.get('headings', {}).get('h1', [])),
                'h2_count': len(content_analysis.get('headings', {}).get('h2', [])),
                'internal_links': len(meta_analysis.get('links', {}).get('internal', [])),
                'external_links': len(meta_analysis.get('links', {}).get('external', []))
            },
            'seo_analysis': {
                'predicted_position': predicted_position,
                'content_metrics': dataclass_to_dict(seo_report.content_metrics),
                'keyword_analysis': dataclass_to_dict(seo_report.keyword_analysis),
                'feature_scores': dataclass_to_dict(seo_report.feature_scores),
                'recommendations': dataclass_to_dict(seo_report.recommendations) if hasattr(seo_report, 'recommendations') else {},
                'priorities': dataclass_to_dict(seo_report.priorities) if hasattr(seo_report, 'priorities') else []
            },
            'raw_page_data': {
                'status_code': status_code,
                'headings': content_analysis.get('headings', {})
            }
        }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON –æ—Ç—á–µ—Ç–∞
        report_file = f'seo-reports/report-{datetime.now().strftime("%Y-%m-%d")}.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f'\nüíæ JSON –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}')

        # –°–æ–∑–¥–∞–Ω–∏–µ Markdown summary
        create_markdown_summary(results)

        print(f'\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –û–±—â–∏–π score: {results["overall_score"]}/100')

    except Exception as e:
        print(f'\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}')
        import traceback
        traceback.print_exc()

        # –°–æ–∑–¥–∞–µ–º error report
        create_error_report({'error': str(e), 'traceback': traceback.format_exc()})
        sys.exit(1)


def create_markdown_summary(results):
    """–°–æ–∑–¥–∞–Ω–∏–µ Markdown –æ—Ç—á–µ—Ç–∞"""
    with open('seo-reports/SUMMARY.md', 'w', encoding='utf-8') as f:
        f.write('# üîç SEO Analysis Report\n\n')
        f.write(f'**Date:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n\n')
        f.write(f'**Site:** {results["site_url"]}\n\n')
        f.write(f'**Overall Score:** {results["overall_score"]}/100\n\n')

        # –°—Ç–∞—Ç—É—Å
        score = results["overall_score"]
        if score >= 80:
            f.write('**Status:** ‚úÖ –û—Ç–ª–∏—á–Ω–æ\n\n')
        elif score >= 60:
            f.write('**Status:** ‚ö†Ô∏è –•–æ—Ä–æ—à–æ, –Ω–æ –µ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è\n\n')
        else:
            f.write('**Status:** ‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n\n')

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        meta = results.get('page_metadata', {})
        f.write('## üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n\n')
        f.write(f'- **Title:** {meta.get("title", "N/A")} ({meta.get("title_length", 0)} —Å–∏–º–≤–æ–ª–æ–≤)\n')
        f.write(f'- **Meta Description:** {"‚úÖ –ï—Å—Ç—å" if meta.get("description") else "‚ùå –ù–µ—Ç"} ({meta.get("description_length", 0)} —Å–∏–º–≤–æ–ª–æ–≤)\n')
        f.write(f'- **H1:** {meta.get("h1_count", 0)} —à—Ç\n')
        f.write(f'- **H2:** {meta.get("h2_count", 0)} —à—Ç\n')
        f.write(f'- **–°–ª–æ–≤:** {meta.get("word_count", 0)}\n')
        f.write(f'- **–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫:** {meta.get("internal_links", 0)}\n')
        f.write(f'- **–í–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫:** {meta.get("external_links", 0)}\n\n')

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        seo_analysis = results.get('seo_analysis', {})
        if isinstance(seo_analysis, dict):
            recommendations_dict = seo_analysis.get('recommendations', {})

            # recommendations –º–æ–∂–µ—Ç –±—ã—Ç—å Dict[str, List[str]] –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            all_recommendations = []
            if isinstance(recommendations_dict, dict):
                for category, recs in recommendations_dict.items():
                    if isinstance(recs, list):
                        all_recommendations.extend(recs)

            if all_recommendations:
                f.write('## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é\n\n')
                for i, rec in enumerate(all_recommendations[:10], 1):
                    f.write(f'{i}. {rec}\n')
                f.write('\n')

        f.write('\n---\n\n')
        f.write('*Powered by SEO AI Models*\n')

    print(f'üìÑ Markdown summary —Å–æ–∑–¥–∞–Ω: seo-reports/SUMMARY.md')


def create_error_report(error_data):
    """–°–æ–∑–¥–∞–Ω–∏–µ error report –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
    try:
        with open('seo-reports/ERROR.md', 'w', encoding='utf-8') as f:
            f.write('# ‚ùå SEO Analysis Error\n\n')
            f.write(f'**Date:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n\n')
            f.write(f'**Site:** {SITE_URL}\n\n')
            f.write('## Error Details\n\n')
            f.write(f'```\n{error_data.get("error", "Unknown error")}\n```\n\n')

            if 'traceback' in error_data:
                f.write('## Traceback\n\n')
                f.write(f'```\n{error_data["traceback"]}\n```\n')

        print('üìù Error report —Å–æ–∑–¥–∞–Ω: seo-reports/ERROR.md')
    except Exception as e:
        print(f'‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å error report: {e}')


if __name__ == "__main__":
    main()
